# Machine Learning
## Sample size and SVM Classification algorithm
- 在实际的工程项目中，分类算法相对变化较少，可以采用一些相对稳定和成熟的工具包，而特征提取则和应用领域紧密相关，变化较多，需要利用领域知识进行设计。这种情况下，模型的性能更大程度上依赖于特征提取而不是分类算法设计，把更多的精力投入到特征提取上是一种常态。
- 并不是说我们拥有大量可用样本，就需要大规模的分类算法，如果面对的分类问题本质上并不需要大量样本，可以对可用样本集进行抽样。之所以需要大规模分类算法，是因为有些问题本质上就需要有大量样本才能解决。设想特征空间里几乎所有的区域都是负类的区域，只有 n 个无规则地分布的正类区域，则解决该问题至少需要 n 个正样本，直观地说，每少一个正样本，性能就会下降 1/n。如果 n 很大，就必须使用大规模的分类算法。
- 当然，一个分类问题所需的样本数并不是和特征提取方法无关的。使用特征 A，也许特征空间里有 n 个正类区域，但使用特征 B，可能特征空间里正类区域就变成了 m 个。所以好的特征提取方法的确也至关重要，但不能因此认为分类方法无关紧要。最理想状态下，可以设计一种一维特征，使得正类负类分别占据坐标轴的正半段和负半段，这样就完全不用分类算法了。可惜这只是一种理想，并不总能成为现实。
- 如果只考虑普通的二分分类问题，大规模分类算法的必要性也许并不明显，目前推动这一领域研究的最主要的是一些更复杂的应用。比如垃圾邮件过滤，虽然是个二分分类问题，但考虑到不同的人对同一邮件是不是垃圾的判断标准不同，需要解决个性化的问题。Yahoo! 或 Google 这样的邮件服务提供者，理论上需要训练几亿个关联的二分分类器，所需的样本量自然巨大。与此相关的问题是个性化搜索，理论上也要为每个用户训练一个 ranker。
- 另外，对于搜索引擎的上下文广告，给定一个查询以及其他一些上下文信息，需要决定展示什么广告给用户。如果过滤掉一些枝节部分，不妨把这个问题看成一个多类分类问题（或者多标记的分类问题），每个广告就是一个类，特征则要从查询及其上下文中去提取。像 Google 这样的服务商所考虑的广告数目应该在百万这个数量级上，而且每天广告种类都会变化，对样本数量的要求当然会非常大。与此相关的问题是对象标注，例如为 flickr 上的照片或者电子商务网站上的商品自动打上 tag。
***
+ 首先：数据集内在的特征。每个数据集就像一个鲜活的生命，有其内在的 DNA，反映了某种本质上的规律和真相。当我们将一个数据集分成训练集和测试集，用训练集训练模型，然后用测试集进行测试和评估是。其背后的逻辑，则是认为部分的数据集也具备整体的特征，因此可用于训练模型。如果我们承认数据集内在的特征。那么就会存在某个算法和模型，相对能更好地描述这个特征，或者反映深层的规律和真相。所以，观察和探索数据集，找到合适的算法，这是首要的。样本的大小也要考虑，但是第二位的。对于一个分类问题的数据集，当用 Logistic 回归、贝叶斯分类或 SVM 等方法来尝试时，会发现不同的方法处于不同的准确度区间。合适的算法，已经决定了大致的准确度。这好比是统计学上的系统误差。剩下的，就是对算法进行具备调优，进一步提高精度了。
+ 样本大小。不同的算法对样本的需求是不同的。基于统计（或者基于内存）的学习算法，相对需要的样本会比较多，因为只有大样本才能形成统计意义的规律。而基于模型的算法，需要的样本相对就少。SVM 需要的样本比较少，因为它的关键是找到支持向量，支持向量之外的远端的样本点，其价值并不大。如果一个很小的样本但是能很好地从中提取支持向量，算法就运行地很棒。相反，贝叶斯分类则需要比较大的样本。从理论来看，Logistic 回归对样本大小的需求介于前二者之间。结合前面讲的第一点，如果我们认为一个数据集适合某种模型，但样本却不够，那就要调整算法，或设法收集更大样本（不过在大数据时代，样本似乎不是问题）。
+ 算法的性能。这包括算法的时间复杂度和空间复杂度，以及模型训练出来后对新样本进行预测的响应时间，还有算法并行化的可能性。
+ 模型的增量学习。如果有新样本的加入，模型可以增量地学习，将新的经验加入，还是必须重新计算？如果发现原先的样本有错误（超出噪声的错误），模型是要重新计算，还是能快速地修正？这方面，基于内存或统计的方法，会有优势。
