# chapter one
- rebots.txt
    大多数网站都会定义 robots.txt 文件，such as www.baidu.com/rebots.txt
    用于介绍该网站爬虫的限制
- sitemap
    定位网站最新的内容，而无须爬取每一个网页
- value site size
    使用谷歌爬虫 www.google.com/advanced_search
     search: site:www.baidu.com
- identity the technologies used by the web site
    builtwith.parse('url')
- search site owners
    python-whois   whois.whois("url")


